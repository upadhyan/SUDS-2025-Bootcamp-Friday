{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF0pPVB4-LHp"
   },
   "source": [
    "\n",
    "# UofT DSI SUDS\n",
    "#### Supervised Learning Lab\n",
    "#### Teaching team: Nakul Upadhya\n",
    "##### Lab author: Kyle E. C. Booth, kbooth@mie.utoronto.ca, edited by Jake Mosseri, Nakul Upadhya, Eldan Cohen, Alex Olson, Shehnaz Islam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3flbTck-LHu"
   },
   "source": [
    "In this lab, we will be introducing *decision tree and forests*. We will introduce the notion of a decision tree, extend this to random forests, and then investigate some state-of-the-art tree-based methods for machine learning. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2033,
     "status": "ok",
     "timestamp": 1715546614109,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "KPXh7nee-LHv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7CedHdx-LHw"
   },
   "source": [
    "### Decision Trees\n",
    "\n",
    "Decision trees are popular supervised learning methods used for classification and regression. The tree represents a series of simple decision rules that predict the target when the feature vector is passed through them. Decision trees are easy to understand, can be visualized nicely, require very little data preparation (e.g., we don't need to scale features), and the trained model can be explained easily to others post priori (as opposed to other *black box* methods that are difficult to communicate).\n",
    "\n",
    "###### Example\n",
    "Suppose you wanted to design a simple decision tree for whether (or not) you buy a used car. You might develop something like the following:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/decision-tree.gif?raw=1\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "litVF00HrKud"
   },
   "source": [
    "**Titanic Survivor Dataset**\n",
    "\n",
    "In this lab, we will be exploring the use of decision trees in the context of Kaggle's famous **Titanic dataset**. Each row in the data represents a passenger, detailing various characteristics about them (i.e., the features), and also details whether or not the passenger survived the disaster.\n",
    "\n",
    "Let's load the data and take a look at it.\n",
    "\n",
    "To get the data into a manageable format, we're going to use the [Pandas](https://pandas.pydata.org/) library, a popular library for data manipulation and analysis. While we won't be providing a full Pandas tutorial, we will provide some insight into key functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "executionInfo": {
     "elapsed": 2632,
     "status": "ok",
     "timestamp": 1715546616740,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "mDl1JKwx-LHx",
    "outputId": "346f9515-b912-468d-f97c-abdb0371a2e8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # import pandas to get access to dataframe operations\n",
    "from sklearn.datasets import fetch_openml # import function to retrieve relevant datasets\n",
    "\n",
    "full_data = fetch_openml(\"titanic\", version=1, as_frame=True) # Get all data and metadata\n",
    "data = full_data.frame # Extract the relevant data\n",
    "data.survived = pd.to_numeric(data['survived'])\n",
    "data.drop(['boat', 'body', 'home.dest'], axis=1, inplace=True) # Drop irrelevant columns\n",
    "data.head() # view the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ulk4gvl-LHz"
   },
   "source": [
    "The above cell used the `fetch_openml` function to pull in the Titanic survivor data. The `.head()` allows us to conveniently take a glance at the first 5 rows (along with the header).\n",
    "\n",
    "We can see that, along with the target 'Survived', we have a number of features including the passenger name, sex, age, fare, cabin, etc. We can do a bit of simple *exploratory data analysis* (EDA) to get a better feel for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1715546616741,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "yDz5AhGs-LHz",
    "outputId": "42f42444-0fbd-4ce8-8783-61da7075ca7c"
   },
   "outputs": [],
   "source": [
    "print (\"Passengers, features: \", data.shape)\n",
    "print (\"Survived: \", data[data[\"survived\"]==1].shape[0], \", Didn't: \", data[data[\"survived\"]==0].shape[0])\n",
    "print (\"female: \", data[data[\"sex\"]==\"female\"].shape[0], \", Male: \", data[data[\"sex\"]==\"male\"].shape[0])\n",
    "print (\"\\n Missing values by feature: \\n\", data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfTLCpGf-LH0"
   },
   "source": [
    "As you can see, we can use Pandas to conveniently summarize key aspects of the dataset such as the number of passengers, features, survived/didn't, and their gender. We are also able to identify the number of missing values per feature in the dataset.\n",
    "\n",
    "To accomplish this, we used Pandas flexible indexing capability. The syntax `data[data[col]==val]` allows us to return the subset of rows in `data` where column `col` takes on value `val`. Very powerful!\n",
    "\n",
    "As you may have suspected, the dataset we're using is actually a subset of the total Titanic data. In reality, there were actually 3,547 passengers while the data we're working with only concerns 1309 of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the passenger survival rate? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[data[\"survived\"]==1].shape[0] / data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many passengers paid more than $10 for fare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[data['fare'] > 10].shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many passengers had a passenger class of 3? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1715546616741,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "3qH9urfd-LH1",
    "outputId": "795fe45d-4a05-40bd-8d24-b84d8f586270"
   },
   "outputs": [],
   "source": [
    "print(data[data['pclass'] == 3].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB5y8KEa-LH2"
   },
   "source": [
    "##### Data Preparation: Categorical -> Numerical Mapping\n",
    "\n",
    "Before we can fit sklearn decision trees to our data, we first need to convert all of the categorical variables (e.g., gender) numerical values - this is called *encoding*. In previous labs, we dealt with datasets that were pre-prepared; now things are getting a little more realistic! Categoricals with unique values (like name and ticket #) can be removed from the dataset entirely as we don't suspect they will contribute to the model.\n",
    "\n",
    "We can do the required preparation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1715546616938,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "0WjofYtW-LH2",
    "outputId": "6a3d5cb8-8e24-43b3-e829-c9b4dadf76ac"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data = data.drop(['name', 'ticket', 'cabin', 'embarked'], axis=1) # remove unimportant columns\n",
    "\n",
    "le = preprocessing.LabelEncoder() # Create a label encoder\n",
    "le.fit(data['sex']) # provide data for it to learn what classes there are\n",
    "data['sex'] = le.transform(data['sex']) # apply the encoding\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-a-uiu0-LH3"
   },
   "source": [
    "In the above cell, we dropped a number of columns we don't suspect will be correlated with the target (*Note: we probably should have been a bit more rigorous about this!*). Then we used the `LabelEncoder()` within sklearn that can fit a numbering scheme to a categorical feature (i.e., 'Sex'). We can see in the new dataset, sex takes on a value of 0 (female) or 1 (male).\n",
    "\n",
    "##### Model Development\n",
    "\n",
    "OK! Let's get to developing some decision tree models to predict passenger survival. We will start with simple decision trees and develop more complex models from there. Our first step, as in previous labs, is to split our data into a training set and a test set (unseen data). We will then use k-folds cross validation on the training set to try and get the best performing model before finally applying it to the test data.\n",
    "\n",
    "Let's import sklearn's decision tree classifer and split the data (using techniques we covered in the first lab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 851,
     "status": "ok",
     "timestamp": 1715546617787,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "uXuxpDkP-LH3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree # import our decision tree model\n",
    "\n",
    "target_data = data[\"survived\"]\n",
    "feature_data = data.iloc[:, data.columns != \"survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data,\n",
    "                                                    target_data,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4U5WwH2-LH4"
   },
   "source": [
    "* How many samples are in the training set? \n",
    "* How many samples are in the test set? \n",
    "* What are the survival rates in each of the datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715546617787,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "ANVOauwm-LH4",
    "outputId": "297d2718-d912-4882-aef5-9bc74ccda098"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(f\"Train survival rate: {y_train[y_train ==1].shape[0] / y_train.shape[0]}\")\n",
    "print(f\"Train survival rate: {y_test[y_test ==1].shape[0] / y_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZp4c8Hg-LH5"
   },
   "source": [
    "##### Dealing with Missing Data: Imputation\n",
    "\n",
    "Before we can fit our decision tree to our training data, we can conduct *imputation* to replace missing values with the mean/median/mode value in the column. For this exercise we will conduct mode imputation (i.e., the most common value in the column).\n",
    "\n",
    "It's important that you don't impute your data using statistics including the the test data! This is an example of *information leak* where your test data is leaking into your training data.\n",
    "\n",
    "As such, we will fit our missing data imputer to our training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1715546617787,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "fQFGk9hk-LH5"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "imp.fit(X_train)\n",
    "X_train = imp.transform(X_train) # replace missing data using our imputer\n",
    "X_test = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqj-hPeg-LH6"
   },
   "source": [
    "##### Fitting a Tree\n",
    "\n",
    "So we've got our data prepared, let's fit a decision tree to our training data.\n",
    "\n",
    "Remember, the pipeline for model development in sklearn is **initialize->fit->predict**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1715546617939,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "VqxAAXVU-LH6",
    "outputId": "1f8bb624-0798-470d-e772-80a424c35ebb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 11)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1yMNmyr-LH6"
   },
   "source": [
    "In the above cell, we defined a Decision Tree classifier and fit it to our training set.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* What is the performance of this model on the test set?\n",
    "* Do you think the model has overfit or underfit: ____________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715546617939,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "F4vftzMR-LH7",
    "outputId": "d1b2e095-8cb2-4706-ca6c-e2cd0e32f148"
   },
   "outputs": [],
   "source": [
    "print(\"Test Accuracy\", accuracy_score(y_test, clf.predict(X_test)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Eo1zcUlKle0"
   },
   "source": [
    "##### Varying Depth\n",
    "\n",
    "Now that we know how to fit a tree, lets see what impact the depth of a tree has on its performance\n",
    "\n",
    "**Your Turn**\n",
    "* Vary the depth of the tree and calculate its train and test accuracy at each depth (Hint: control the maximumum depth using the `max_depth` parameter\n",
    "* Plot the train and test accuracy together.\n",
    "* At what depth is the model underfitting and at what depth is the model overfitting? _______________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 1263,
     "status": "ok",
     "timestamp": 1715546619201,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "MZhq-FMYKk0n",
    "outputId": "2780d516-6f3b-4922-be3f-3498b9393686"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "depths = list(range(2,11))\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for depth in depths:\n",
    "  clf = tree.DecisionTreeClassifier(max_depth = depth)\n",
    "  clf.fit(X_train, y_train)\n",
    "  accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "  test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "  train_accs.append(accuracy)\n",
    "  test_accs.append(test_acc)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(depths, train_accs)\n",
    "ax.plot(depths, test_accs)\n",
    "ax.set_title('Model Performance across Depth')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HL_BERf-LH8"
   },
   "source": [
    "\n",
    "##### Visualizing the Tree\n",
    "\n",
    "One useful thing we can do is actually visualize our decision tree model! We can use the [graphViz](https://www.graphviz.org/) library to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1715546619577,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "nQ2xJfJm-LH8",
    "outputId": "7b4d9214-3503-4acb-f09e-96978a129093"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf, filled=True, feature_names=data.columns.drop('survived'), fontsize=10) # Plot the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EgolEWB-LH8"
   },
   "source": [
    "Lets explore the decision tree and answer the following:\n",
    "* What feature does the root node split on?\n",
    "* What features appear most in the tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2gqE8y3_yXP"
   },
   "source": [
    "##### HP Tuning\n",
    "There are many hyper-parameters that can be tuned to change how the model performs. Some common parameters that are modified include:\n",
    "1. Max Tree Depth: How \"tall\" do you want your tree to be\n",
    "2. Minimum Samples Per Leaf: This parameter defines the minimum number of training datapoints that fall into a given leaf node in order for that node to be created\n",
    "3. Minimum Samples to Split: This parameter controls the minimum number of samples required to create a decision split\n",
    "\n",
    "To decide the values of each of the parameters, we can use Grid Search combined with cross validation. In Grid Search, we first decide what potential values we want each hyperparameter will take. Then we find every possible combination of parameters and run cross validation on each combination to estimate the performance of that hyperparameter combination.\n",
    "\n",
    "Luckily, `sklearn` has a nice implementation of Grid Search that runs this algorithm for us.\n",
    "\n",
    "Here we want to tune three parameters: max_depth, min_samples_split, and min_samples_leaf. To do this, we need to define possible values we want to search over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1070,
     "status": "ok",
     "timestamp": 1715546620643,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "zmpcYAMI_xyX",
    "outputId": "68253c05-3c95-4692-a785-9a0f75e18a41"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "clf = tree.DecisionTreeClassifier() # First we define our model without passing in parameters\n",
    "hyperparameter_search = { # Then we decide the possible parameter combinations\n",
    "    'max_depth': [2,4,6], ## FILL THIS IN\n",
    "    'min_samples_split': [2, 5, 8],\n",
    "    'min_samples_leaf': [2,5,8] ## FILL THIS IN\n",
    "}\n",
    "evaluation_metric = make_scorer(accuracy_score, # GridSearchCV requires us to wrap our metric function in a \"scorer\"\n",
    "                                greater_is_better = True)\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator = clf,\n",
    "                              param_grid = hyperparameter_search,\n",
    "                              scoring = evaluation_metric,\n",
    "                              cv = 5) # Set up search algorithm\n",
    "grid_search_cv.fit(X_train, y_train) # Run the search. NOTE: This may take a while\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_cv.best_params_) # Print the parameters\n",
    "print (\"Best CV Accuracy: \", grid_search_cv.best_score_ * 100, \"%\")\n",
    "\n",
    "clf = grid_search_cv.best_estimator_ # Get the best model from the GridSearch\n",
    "accuracy = accuracy_score(y_test, clf.predict(imp.transform(X_test)))\n",
    "print (\"Testing Accuracy: \", accuracy * 100, \"%\") # Print the testing accuracy of the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qrr1hd59EiVK"
   },
   "source": [
    "In the cell above, we tested our three values per hyperparameter and ran grid search to find the best combination from the space we defined. As you may have noticed, the number of combinations tested by Grid Search exponentially increases as you test more values and tune more hyperparameters. This means that performing a grid search is often a task that takes a long period of time and is often **not** used for more complex models like neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlFxKh8YQtnT"
   },
   "source": [
    "### Other Models\n",
    "If you have finished early, feel free to try other models and try to get as high of a test accuracy as you can!\n",
    "\n",
    "Some other models you can start with:\n",
    "1. [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "2. [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "3. [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "You can also use models from other packages. Some examples include:\n",
    "1. [XGBoost](https://xgboost.readthedocs.io/en/stable/get_started.html) (you will need to run `!pip install xgboost`)\n",
    "2. [LightGBM](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier) (`!pip install lightgbm`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715546620644,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "rbx6Ut2vQwWI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "IncepTree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
